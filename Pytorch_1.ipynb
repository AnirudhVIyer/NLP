{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4pvCGdUflCKRC9mfFDHQ9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnirudhVIyer/NLP/blob/main/Pytorch_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MSAdBT8xVUmw"
      },
      "outputs": [],
      "source": [
        "# create data\n",
        "import numpy as np\n",
        "np.random.seed(99)\n",
        "x = np.random.rand(100, 1)\n",
        "y = 1 + 2 * x + .1 * np.random.randn(100, 1) #actual a=1 b=2\n",
        "\n",
        "# Shuffles the indices\n",
        "idx = np.arange(100)\n",
        "np.random.shuffle(idx)\n",
        "\n",
        "# Uses first 80 random indices for train\n",
        "train_idx = idx[:80]\n",
        "# Uses the remaining indices for validation\n",
        "val_idx = idx[80:]\n",
        "\n",
        "# Generates train and validation sets\n",
        "x_train, y_train = x[train_idx], y[train_idx]\n",
        "x_val, y_val = x[val_idx], y[val_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we know a=1, b=2, let's validate it throughout the process"
      ],
      "metadata": {
        "id": "bHr2zJxj7HFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializes parameters \"a\" and \"b\" randomly\n",
        "np.random.seed(42)\n",
        "a = np.random.randn(1)\n",
        "b = np.random.randn(1)\n",
        "\n",
        "print(a, b)\n",
        "\n",
        "# Sets learning rate\n",
        "lr = 1e-1\n",
        "# Defines number of epochs\n",
        "n_epochs = 1000\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # Computes our model's predicted output\n",
        "    yhat = a + b * x_train\n",
        "    \n",
        "    # How wrong is our model? That's the error! \n",
        "    error = (y_train - yhat)\n",
        "    # It is a regression, so it computes mean squared error (MSE)\n",
        "    loss = (error ** 2).mean()\n",
        "    \n",
        "    # Computes gradients for both \"a\" and \"b\" parameters\n",
        "    a_grad = -2 * error.mean()\n",
        "    b_grad = -2 * (x_train * error).mean()\n",
        "    \n",
        "    # Updates parameters using gradients and the learning rate\n",
        "    a = a - lr * a_grad\n",
        "    b = b - lr * b_grad\n",
        "    \n",
        "print(a, b)\n",
        "\n",
        "# Sanity Check: do we get the same results as our gradient descent?\n",
        "from sklearn.linear_model import LinearRegression\n",
        "linr = LinearRegression()\n",
        "linr.fit(x_train, y_train)\n",
        "print(linr.intercept_, linr.coef_[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14zZ_niKWmOR",
        "outputId": "4ef7964e-23d4-4fb6-f652-3b87b62103a3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.49671415] [-0.1382643]\n",
            "[1.07891506] [1.86215196]\n",
            "[1.07891418] [1.8621536]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e#40de"
      ],
      "metadata": {
        "id": "HRDt8x5pW-OB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "!pip install torchviz\n",
        "from torchviz import make_dot\n",
        "\n",
        "# what processor to use\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\n",
        "# and then we send them to the chosen device\n",
        "x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
        "y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
        "\n",
        "# Here we can see the difference - notice that .type() is more useful\n",
        "# since it also tells us WHERE the tensor is (device)\n",
        "print(type(x_train), type(x_train_tensor), x_train_tensor.type())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKe5yRd6W_AE",
        "outputId": "a3a901a5-0fd0-4ca5-f24b-302ba2a1232b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.8/dist-packages (0.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from torchviz) (1.13.0+cu116)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.8/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->torchviz) (4.4.0)\n",
            "<class 'numpy.ndarray'> <class 'torch.Tensor'> torch.cuda.FloatTensor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FIRST\n",
        "# Initializes parameters \"a\" and \"b\" randomly, ALMOST as we did in Numpy\n",
        "# since we want to apply gradient descent on these parameters, we need\n",
        "# to set REQUIRES_GRAD = TRUE\n",
        "# cpu tensor\n",
        "a = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
        "b = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
        "print(a, b)\n",
        "\n",
        "# SECOND\n",
        "# But what if we want to run it on a GPU? We could just send them to device, right?\n",
        "# send to gpu but lose gradients\n",
        "a = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
        "b = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
        "print(a, b)\n",
        "# Sorry, but NO! The to(device) \"shadows\" the gradient...\n",
        "\n",
        "# THIRD\n",
        "# We can either create regular tensors and send them to the device (as we did with our data)\n",
        "a = torch.randn(1, dtype=torch.float).to(device)\n",
        "b = torch.randn(1, dtype=torch.float).to(device)\n",
        "# and THEN set them as requiring gradients...\n",
        "a.requires_grad_()\n",
        "b.requires_grad_()\n",
        "print(a, b)\n",
        "\n",
        "## all these methods have their issues"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeaJeZ1AYoCJ",
        "outputId": "d65bbddd-965a-48e8-f469-c0412b24111e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.2318], requires_grad=True) tensor([0.8427], requires_grad=True)\n",
            "tensor([0.9739], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([-0.8145], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "tensor([0.1432], device='cuda:0', requires_grad=True) tensor([0.6174], device='cuda:0', requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## bestter way\n",
        "## create a tensor, require compute gradient and dorectly sent it tp a devide\n",
        "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "print(a, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H89AbaFOdJDc",
        "outputId": "2595706f-9833-4e7f-cc1e-0e7b29ca1ab2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.1342], device='cuda:0', requires_grad=True) tensor([-0.1530], device='cuda:0', requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-1\n",
        "n_epochs = 1000\n",
        "\n",
        "torch.manual_seed(42)\n",
        "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    yhat = a + b * x_train_tensor\n",
        "    error = y_train_tensor - yhat\n",
        "    loss = (error ** 2).mean()\n",
        "\n",
        "    # No more manual computation of gradients! \n",
        "    # a_grad = -2 * error.mean()\n",
        "    # b_grad = -2 * (x_tensor * error).mean()\n",
        "    \n",
        "    # We just tell PyTorch to work its way BACKWARDS from the specified loss!\n",
        "    loss.backward()\n",
        "    # Let's check the computed gradients...\n",
        "    # print(a.grad)\n",
        "    # print(b.grad)\n",
        "    \n",
        "    # What about UPDATING the parameters? Not so fast...\n",
        "    \n",
        "    # FIRST ATTEMPT\n",
        "    # AttributeError: 'NoneType' object has no attribute 'zero_'\n",
        "    # a = a - lr * a.grad\n",
        "    # b = b - lr * b.grad\n",
        "    # print(a)\n",
        "\n",
        "    # SECOND ATTEMPT\n",
        "    # RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.\n",
        "    # a -= lr * a.grad\n",
        "    # b -= lr * b.grad        \n",
        "    \n",
        "    # THIRD ATTEMPT\n",
        "    # We need to use NO_GRAD to keep the update out of the gradient computation\n",
        "    # Why is that? It boils down to the DYNAMIC GRAPH that PyTorch uses...\n",
        "    with torch.no_grad():\n",
        "        a -= lr * a.grad\n",
        "        b -= lr * b.grad\n",
        "    \n",
        "    # PyTorch is \"clingy\" to its computed gradients, we need to tell it to let it go...\n",
        "    a.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "    \n",
        "print(a, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajoi2O67gz39",
        "outputId": "cc4aafc0-fa9e-4991-b840-10aae29aa73c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.0789], device='cuda:0', requires_grad=True) tensor([1.8621], device='cuda:0', requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## using optimizers and loss\n",
        "torch.manual_seed(42)\n",
        "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "print(a, b)\n",
        "\n",
        "lr = 1e-1\n",
        "n_epochs = 1000\n",
        "\n",
        "# Defines a MSE loss function\n",
        "loss_fn = nn.MSELoss(reduction='mean')\n",
        "\n",
        "# Defines a SGD optimizer to update the parameters\n",
        "optimizer = optim.SGD([a, b], lr=lr)\n",
        "# adam exampls : optimizer = optim.Adam([a,b],lr-lr)\n",
        "for epoch in range(n_epochs):\n",
        "    yhat = a + b * x_train_tensor\n",
        "    error = y_train_tensor - yhat\n",
        "    loss = loss_fn(y_train_tensor,yhat)\n",
        "\n",
        "    loss.backward()    \n",
        "    \n",
        "    # No more manual update!\n",
        "    # with torch.no_grad():\n",
        "    #     a -= lr * a.grad\n",
        "    #     b -= lr * b.grad\n",
        "    optimizer.step()\n",
        "    \n",
        "    # No more telling PyTorch to let gradients go!\n",
        "    # a.grad.zero_()\n",
        "    # b.grad.zero_()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "print(a, b)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jh3G4RnBfwhy",
        "outputId": "c26e126e-ae3d-4d21-fa23-6b3ad04d0968"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1940], device='cuda:0', requires_grad=True) tensor([0.1391], device='cuda:0', requires_grad=True)\n",
            "tensor([1.0789], device='cuda:0', requires_grad=True) tensor([1.8621], device='cuda:0', requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## creating a model class\n",
        "\n",
        "class customModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.a = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float, device=device))\n",
        "    self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float, device=device))\n",
        "    \n",
        "  def forward(self, x):\n",
        "    # Computes the outputs / predictions\n",
        "     return self.a + self.b * x\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Jue1sxY1he8Z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "##create a model and send to device\n",
        "new_model = customModel().to(device)\n",
        "\n",
        "lr = 1e-1\n",
        "n_epochs = 1000\n",
        "\n",
        "# Defines a MSE loss function\n",
        "loss_fn = nn.MSELoss(reduction='mean')\n",
        "\n",
        "# Defines a SGD optimizer to update the parameters\n",
        "optimizer = optim.SGD(new_model.parameters(), lr=lr)\n",
        "# adam exampls : optimizer = optim.Adam([a,b],lr-lr)\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    #setup train mode\n",
        "    new_model.train()\n",
        "    yhat = new_model(x_train_tensor)\n",
        "\n",
        "    loss = loss_fn(y_train_tensor,yhat)\n",
        "    loss.backward()    \n",
        "    \n",
        "    # No more manual update!\n",
        "    # with torch.no_grad():\n",
        "    #     a -= lr * a.grad\n",
        "    #     b -= lr * b.grad\n",
        "    optimizer.step()\n",
        "    \n",
        "    # No more telling PyTorch to let gradients go!\n",
        "    # a.grad.zero_()\n",
        "    # b.grad.zero_()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "print(new_model.state_dict())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIvFuPTBSHtx",
        "outputId": "33f3f4f3-2086-41f4-84fb-0f15737fd2df"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('a', tensor([1.0789], device='cuda:0')), ('b', tensor([1.8621], device='cuda:0'))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_train_step(model,lossfn,optimizer):\n",
        "  def train_step(x,y):\n",
        "    model.train()\n",
        "    yhat = model(x)\n",
        "    loss = lossfn(y,yhat)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    return loss.item()\n",
        "\n",
        "  return train_step\n",
        "\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "##create a model and send to device\n",
        "new_model = customModel().to(device)\n",
        "\n",
        "lr = 1e-1\n",
        "n_epochs = 1000\n",
        "\n",
        "# Defines a MSE loss function\n",
        "loss_fn = nn.MSELoss(reduction='mean')\n",
        "\n",
        "# Defines a SGD optimizer to update the parameters\n",
        "optimizer = optim.SGD(new_model.parameters(), lr=lr)\n",
        "# adam exampls : optimizer = optim.Adam([a,b],lr-lr)\n",
        "losses= []\n",
        "train_step = make_train_step(new_model,loss_fn,optimizer)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "      # Performs one train step and returns the corresponding loss\n",
        "    loss = train_step(x_train_tensor, y_train_tensor)\n",
        "    losses.append(loss)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#print(losses[-1])\n",
        "print(new_model.state_dict())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwXRSeZrS-XH",
        "outputId": "3180920d-ecca-4078-cc21-a1cf0c58d077"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('a', tensor([1.0789], device='cuda:0')), ('b', tensor([1.8621], device='cuda:0'))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Builing Dataset and Dataloader\n",
        "\n",
        "from torch.utils.data import Dataset, TensorDataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, x_tensor, y_tensor):\n",
        "        self.x = x_tensor\n",
        "        self.y = y_tensor\n",
        "        \n",
        "    def __getitem__(self, index): ##to get specific index from the data\n",
        "        return (self.x[index], self.y[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "# Wait, is this a CPU tensor now? Why? Where is .to(device)?\n",
        "x_train_tensor = torch.from_numpy(x_train).float()\n",
        "y_train_tensor = torch.from_numpy(y_train).float()\n",
        "\n",
        "train_data = CustomDataset(x_train_tensor, y_train_tensor) \n",
        "print(train_data[0])##example of __getitem__\n",
        "\n",
        "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "print(train_data[0])\n",
        "\n",
        "## here we have crated the x,y data and stored it as a dataset in CPU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdIYv-9D1BAW",
        "outputId": "f87fe09f-e22a-4c05-d5d5-ceb043c1a421"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([0.5543]), tensor([2.1181]))\n",
            "(tensor([0.5543]), tensor([2.1181]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## use a dataloader class to wrap this dataset to serve mini batches\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)\n",
        "losses = []\n",
        "\n",
        "train_step = make_train_step(new_model, loss_fn, optimizer)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        # the dataset \"lives\" in the CPU, so do our mini-batches\n",
        "        # therefore, we need to send those mini-batches to the\n",
        "        # device where the model \"lives\"\n",
        "        x_batch = x_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "        \n",
        "        loss = train_step(x_batch, y_batch)\n",
        "        losses.append(loss)\n",
        "        \n",
        "print(new_model.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DogxkIlu37Pi",
        "outputId": "f83e21e2-d4ad-4b4f-be0d-fcb97bd6b135"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('a', tensor([1.0796], device='cuda:0')), ('b', tensor([1.8619], device='cuda:0'))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with random splitter \n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "x_tensor = torch.from_numpy(x).float()\n",
        "y_tensor = torch.from_numpy(y).float()\n",
        "\n",
        "dataset = TensorDataset(x_tensor, y_tensor)\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [80, 20])\n",
        "\n",
        "# each dataloader for different datasets\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=16)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=20)"
      ],
      "metadata": {
        "id": "NKWKqjiI40jW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### with validation evaluation\n",
        "\n",
        "\n",
        "losses = []\n",
        "val_losses = []\n",
        "train_step = make_train_step(new_model, loss_fn, optimizer)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  #train one batch\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        x_batch = x_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        loss = train_step(x_batch, y_batch)\n",
        "        losses.append(loss)\n",
        "\n",
        "        \n",
        "    with torch.no_grad():\n",
        "      #validate that batch\n",
        "        for x_val, y_val in val_loader:\n",
        "            x_val = x_val.to(device)\n",
        "            y_val = y_val.to(device)\n",
        "            \n",
        "            new_model.eval() ## in eval mode, will not learn from it\n",
        "\n",
        "            yhat = new_model(x_val)\n",
        "            val_loss = loss_fn(y_val, yhat)\n",
        "            val_losses.append(val_loss.item()) #val loss won't affect the parameters, but for our reference\n",
        "\n",
        "   \n",
        "\n",
        "print(new_model.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyVW-fNn5EPE",
        "outputId": "a9e811fb-08f2-4670-faf1-16919cc6b295"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('a', tensor([1.0515], device='cuda:0')), ('b', tensor([1.9164], device='cuda:0'))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "\n",
        "class ManualLinearRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "def make_train_step(model, loss_fn, optimizer):\n",
        "    def train_step(x, y):\n",
        "        model.train()\n",
        "        yhat = model(x)\n",
        "        loss = loss_fn(y, yhat)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        return loss.item()\n",
        "    return train_step\n",
        "\n",
        "\n",
        "model = ManualLinearRegression().to(device) # model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
        "loss_fn = nn.MSELoss(reduction='mean')\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-1)\n",
        "train_step = make_train_step(model, loss_fn, optimizer)\n",
        "\n",
        "n_epochs = 100\n",
        "training_losses = []\n",
        "validation_losses = []\n",
        "print(model.state_dict())\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    batch_losses = []\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        x_batch = x_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "        loss = train_step(x_batch, y_batch)\n",
        "        batch_losses.append(loss)\n",
        "    training_loss = np.mean(batch_losses)\n",
        "    training_losses.append(training_loss)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        val_losses = []\n",
        "        for x_val, y_val in val_loader:\n",
        "            x_val = x_val.to(device)\n",
        "            y_val = y_val.to(device)\n",
        "            model.eval()\n",
        "            yhat = model(x_val)\n",
        "            val_loss = loss_fn(y_val, yhat).item()\n",
        "            val_losses.append(val_loss)\n",
        "        validation_loss = np.mean(val_losses)\n",
        "        validation_losses.append(validation_loss)\n",
        "\n",
        "    print(f\"[{epoch+1}] Training loss: {training_loss:.3f}\\t Validation loss: {validation_loss:.3f}\")\n",
        "\n",
        "print(model.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeClW5x75z9A",
        "outputId": "cbf4deac-fd7d-458a-881f-be9656584f57"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('linear.weight', tensor([[0.7645]], device='cuda:0')), ('linear.bias', tensor([0.8300], device='cuda:0'))])\n",
            "[1] Training loss: 0.354\t Validation loss: 0.071\n",
            "[2] Training loss: 0.071\t Validation loss: 0.038\n",
            "[3] Training loss: 0.051\t Validation loss: 0.033\n",
            "[4] Training loss: 0.046\t Validation loss: 0.029\n",
            "[5] Training loss: 0.041\t Validation loss: 0.026\n",
            "[6] Training loss: 0.037\t Validation loss: 0.023\n",
            "[7] Training loss: 0.033\t Validation loss: 0.021\n",
            "[8] Training loss: 0.030\t Validation loss: 0.018\n",
            "[9] Training loss: 0.027\t Validation loss: 0.016\n",
            "[10] Training loss: 0.025\t Validation loss: 0.015\n",
            "[11] Training loss: 0.023\t Validation loss: 0.014\n",
            "[12] Training loss: 0.021\t Validation loss: 0.012\n",
            "[13] Training loss: 0.019\t Validation loss: 0.011\n",
            "[14] Training loss: 0.018\t Validation loss: 0.011\n",
            "[15] Training loss: 0.017\t Validation loss: 0.010\n",
            "[16] Training loss: 0.016\t Validation loss: 0.009\n",
            "[17] Training loss: 0.015\t Validation loss: 0.009\n",
            "[18] Training loss: 0.014\t Validation loss: 0.009\n",
            "[19] Training loss: 0.013\t Validation loss: 0.008\n",
            "[20] Training loss: 0.013\t Validation loss: 0.008\n",
            "[21] Training loss: 0.012\t Validation loss: 0.008\n",
            "[22] Training loss: 0.012\t Validation loss: 0.008\n",
            "[23] Training loss: 0.011\t Validation loss: 0.008\n",
            "[24] Training loss: 0.011\t Validation loss: 0.007\n",
            "[25] Training loss: 0.011\t Validation loss: 0.007\n",
            "[26] Training loss: 0.011\t Validation loss: 0.007\n",
            "[27] Training loss: 0.010\t Validation loss: 0.007\n",
            "[28] Training loss: 0.010\t Validation loss: 0.007\n",
            "[29] Training loss: 0.010\t Validation loss: 0.007\n",
            "[30] Training loss: 0.010\t Validation loss: 0.007\n",
            "[31] Training loss: 0.010\t Validation loss: 0.007\n",
            "[32] Training loss: 0.010\t Validation loss: 0.007\n",
            "[33] Training loss: 0.009\t Validation loss: 0.007\n",
            "[34] Training loss: 0.009\t Validation loss: 0.007\n",
            "[35] Training loss: 0.009\t Validation loss: 0.007\n",
            "[36] Training loss: 0.009\t Validation loss: 0.007\n",
            "[37] Training loss: 0.009\t Validation loss: 0.008\n",
            "[38] Training loss: 0.009\t Validation loss: 0.008\n",
            "[39] Training loss: 0.009\t Validation loss: 0.008\n",
            "[40] Training loss: 0.009\t Validation loss: 0.008\n",
            "[41] Training loss: 0.009\t Validation loss: 0.008\n",
            "[42] Training loss: 0.009\t Validation loss: 0.008\n",
            "[43] Training loss: 0.009\t Validation loss: 0.008\n",
            "[44] Training loss: 0.009\t Validation loss: 0.008\n",
            "[45] Training loss: 0.009\t Validation loss: 0.008\n",
            "[46] Training loss: 0.009\t Validation loss: 0.008\n",
            "[47] Training loss: 0.009\t Validation loss: 0.008\n",
            "[48] Training loss: 0.009\t Validation loss: 0.008\n",
            "[49] Training loss: 0.009\t Validation loss: 0.008\n",
            "[50] Training loss: 0.009\t Validation loss: 0.008\n",
            "[51] Training loss: 0.009\t Validation loss: 0.008\n",
            "[52] Training loss: 0.009\t Validation loss: 0.008\n",
            "[53] Training loss: 0.009\t Validation loss: 0.008\n",
            "[54] Training loss: 0.009\t Validation loss: 0.008\n",
            "[55] Training loss: 0.009\t Validation loss: 0.008\n",
            "[56] Training loss: 0.009\t Validation loss: 0.008\n",
            "[57] Training loss: 0.009\t Validation loss: 0.008\n",
            "[58] Training loss: 0.009\t Validation loss: 0.008\n",
            "[59] Training loss: 0.009\t Validation loss: 0.008\n",
            "[60] Training loss: 0.009\t Validation loss: 0.008\n",
            "[61] Training loss: 0.009\t Validation loss: 0.008\n",
            "[62] Training loss: 0.009\t Validation loss: 0.008\n",
            "[63] Training loss: 0.009\t Validation loss: 0.008\n",
            "[64] Training loss: 0.009\t Validation loss: 0.008\n",
            "[65] Training loss: 0.009\t Validation loss: 0.008\n",
            "[66] Training loss: 0.009\t Validation loss: 0.008\n",
            "[67] Training loss: 0.009\t Validation loss: 0.008\n",
            "[68] Training loss: 0.009\t Validation loss: 0.008\n",
            "[69] Training loss: 0.009\t Validation loss: 0.008\n",
            "[70] Training loss: 0.009\t Validation loss: 0.008\n",
            "[71] Training loss: 0.009\t Validation loss: 0.008\n",
            "[72] Training loss: 0.009\t Validation loss: 0.008\n",
            "[73] Training loss: 0.009\t Validation loss: 0.008\n",
            "[74] Training loss: 0.009\t Validation loss: 0.008\n",
            "[75] Training loss: 0.009\t Validation loss: 0.008\n",
            "[76] Training loss: 0.009\t Validation loss: 0.008\n",
            "[77] Training loss: 0.009\t Validation loss: 0.008\n",
            "[78] Training loss: 0.009\t Validation loss: 0.008\n",
            "[79] Training loss: 0.009\t Validation loss: 0.008\n",
            "[80] Training loss: 0.009\t Validation loss: 0.008\n",
            "[81] Training loss: 0.009\t Validation loss: 0.008\n",
            "[82] Training loss: 0.009\t Validation loss: 0.008\n",
            "[83] Training loss: 0.009\t Validation loss: 0.008\n",
            "[84] Training loss: 0.009\t Validation loss: 0.008\n",
            "[85] Training loss: 0.009\t Validation loss: 0.008\n",
            "[86] Training loss: 0.009\t Validation loss: 0.008\n",
            "[87] Training loss: 0.009\t Validation loss: 0.008\n",
            "[88] Training loss: 0.009\t Validation loss: 0.008\n",
            "[89] Training loss: 0.009\t Validation loss: 0.008\n",
            "[90] Training loss: 0.009\t Validation loss: 0.008\n",
            "[91] Training loss: 0.009\t Validation loss: 0.008\n",
            "[92] Training loss: 0.009\t Validation loss: 0.008\n",
            "[93] Training loss: 0.009\t Validation loss: 0.008\n",
            "[94] Training loss: 0.009\t Validation loss: 0.008\n",
            "[95] Training loss: 0.009\t Validation loss: 0.008\n",
            "[96] Training loss: 0.009\t Validation loss: 0.008\n",
            "[97] Training loss: 0.009\t Validation loss: 0.008\n",
            "[98] Training loss: 0.009\t Validation loss: 0.008\n",
            "[99] Training loss: 0.009\t Validation loss: 0.008\n",
            "[100] Training loss: 0.009\t Validation loss: 0.008\n",
            "OrderedDict([('linear.weight', tensor([[1.9156]], device='cuda:0')), ('linear.bias', tensor([1.0519], device='cuda:0'))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9WUeue2a7Cz8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}