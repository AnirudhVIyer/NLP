# -*- coding: utf-8 -*-
"""imdb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13-XJweDuwKvq9NyeK1FfkUx4HNTB2eOZ
"""

import pandas as pd
import numpy as np
import keras
import nltk

train = pd.read_csv('Train.csv')
test = pd.read_csv('Test.csv')

## here each review may have many words, we will create a corpus of only the top 5000 words
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
import re
train_new = []
vocab_size = 5000
stemmer = PorterStemmer()

for i in range(len(train.text)):
    text = re.sub('[^a-zA-Z]',' ',train.text[i])
    text = text.lower()
    #text = text.split()
    
    
    #words = [stemmer.stem(word) for word in text if word not in stopwords.words('english')]
    
    train_new.append(text)

train['text_new'] = train_new
train.drop('text',axis=1)
train['text'] = train['text_new']
train.drop('text_new',axis = 1)

vocab_size = 5000
from keras_preprocessing.text import one_hot
onehotrep = [one_hot(review,vocab_size) for review in train.text]

## find avg words in a review
mean = 0
for i in range(len(onehotrep)):
    mean = mean + len(onehotrep[i])

avg_words = mean/len(onehotrep)
print(avg_words)

## padding 
from keras.preprocessing.sequence import pad_sequences
max_length = 300
embedded_docs = pad_sequences(onehotrep,padding='pre',maxlen = max_length)

## building a model 
from keras.layers import Dense,Embedding,Dropout,LSTM
from keras.models import Sequential

features_measured = 200
model = Sequential()
model.add(Embedding(vocab_size,features_measured,input_length=max_length))  


model.add(LSTM(100))


model.add(Dense(1,activation='sigmoid'))

## here the size of the vocab , the no. of feature we want it to be represented against
model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

from sklearn.model_selection import train_test_split
xtrain,xtest,ytrain,ytest = train_test_split(embedded_docs,train.label,test_size=0.2,random_state=1)

model.fit(xtrain,ytrain,validation_data=(xtest,ytest),batch_size=40,epochs=5)

## here each review may have many words, we will create a corpus of only the top 5000 words
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
import re
test_new = []
vocab_size = 5000
stemmer = PorterStemmer()

for i in range(len(test.text)):
    text = re.sub('[^a-zA-Z]',' ',test.text[i])
    text = text.lower()
    #text = text.split()
    
    
    #words = [stemmer.stem(word) for word in text if word not in stopwords.words('english')]
    
    test_new.append(text)

test['text_new'] = test_new
test.drop('text',axis=1)
test['text'] = test['text_new']
test.drop('text_new',axis = 1)

vocab_size = 5000
from keras_preprocessing.text import one_hot
onehotrep_test = [one_hot(review,vocab_size) for review in test.text]

## padding 
from keras.preprocessing.sequence import pad_sequences
max_length = 300
embedded_docs_test = pad_sequences(onehotrep_test,padding='pre',maxlen = max_length)

yhat = model.predict(embedded_docs_test)

yhat = (yhat>0.5)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(yhat,test.label)
cm

model.evaluate(embedded_docs_test,test.label)

model.save('IMDB_classifier.h5')



new_model = keras.models.load_model('IMDB_classifier.h5')

new_model.evaluate(embedded_docs_test,test.label)

model.summary()

new_model.evaluate(embedded_docs_test,test.label)

model